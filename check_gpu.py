#!/usr/bin/env python3
"""
GPUæ£€æµ‹è„šæœ¬
"""

import sys
import torch

def check_gpu():
    print("=" * 50)
    print("GPU å…¼å®¹æ€§æ£€æµ‹")
    print("=" * 50)
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if torch.cuda.is_available():
        print("âœ… CUDA å¯ç”¨")
        
        # è·å–GPUä¿¡æ¯
        device_count = torch.cuda.device_count()
        print(f"GPU æ•°é‡: {device_count}")
        
        for i in range(device_count):
            print(f"\nGPU {i}:")
            print(f"  åç§°: {torch.cuda.get_device_name(i)}")
            print(f"  CUDAèƒ½åŠ›: {torch.cuda.get_device_capability(i)}")
            
            # VRAMä¿¡æ¯
            props = torch.cuda.get_device_properties(i)
            print(f"  æ€»VRAM: {props.total_memory / 1024**3:.1f} GB")
            print(f"  å¤šå¤„ç†å™¨: {props.multi_processor_count}")
        
        # æµ‹è¯•å¼ é‡è®¡ç®—
        print("\nğŸ§ª æ€§èƒ½æµ‹è¯•...")
        try:
            a = torch.randn(10000, 10000).cuda()
            b = torch.randn(10000, 10000).cuda()
            result = torch.matmul(a, b)
            print("âœ… GPU è®¡ç®—æµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"âŒ GPU è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            
    else:
        print("âŒ CUDA ä¸å¯ç”¨")
        print("\nå¯èƒ½çš„åŸå› :")
        print("1. æ²¡æœ‰NVIDIA GPU")
        print("2. æœªå®‰è£…NVIDIAé©±åŠ¨")
        print("3. PyTorchæœªå®‰è£…CUDAç‰ˆæœ¬")
        print("\nè§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥GPU: nvidia-smi")
        print("2. å®‰è£…CUDAç‰ˆæœ¬PyTorch:")
        print("   pip uninstall torch torchvision torchaudio")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    # æ£€æŸ¥faster-whisperå…¼å®¹æ€§
    print("\n" + "=" * 50)
    print("faster-whisper å…¼å®¹æ€§")
    print("=" * 50)
    
    try:
        from faster_whisper import WhisperModel
        print("âœ… faster-whisper å·²å®‰è£…")
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        print("æµ‹è¯•æ¨¡å‹åŠ è½½...")
        model = WhisperModel("tiny", device="cpu", compute_type="float32")
        print("âœ… æ¨¡å‹åŠ è½½æµ‹è¯•é€šè¿‡")
        
        # GPUæµ‹è¯•
        if torch.cuda.is_available():
            print("\næµ‹è¯•GPUæ¨¡å‹åŠ è½½...")
            try:
                model = WhisperModel("tiny", device="cuda", compute_type="float16")
                print("âœ… GPUæ¨¡å‹åŠ è½½æµ‹è¯•é€šè¿‡")
            except Exception as e:
                print(f"âŒ GPUæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                
    except ImportError:
        print("âŒ faster-whisper æœªå®‰è£…")
        print("å®‰è£…å‘½ä»¤: pip install faster-whisper")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    check_gpu()
    input("\næŒ‰å›è½¦é”®é€€å‡º...")